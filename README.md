# Deep Learning Lab Experiments
## Table of Contents
- Lab 2: Classifier on Weather Dataset
- Lab 3: Classifier on PIMA Diabetes Dataset
- Lab 4: Feed Forward Neural Network on MNIST
- Lab 5: Activation Functions
- Lab 6 & 7: Comparison of Deep and Shallow Autoencoders
- Lab 8: Variational Autoencoder on MNIST
- Lab 9: Experiment with LSTM
- Lab 10: Compression on MNIST with Autoencoder
- Lab 11: Recurrent Neural Network
- Lab 12: Deep Convolutional GAN
- Lab 14: Transfer Learning with Pre-trained CNN

---

### Lab Descriptions

#### Lab 2: Classifier on Weather Dataset
**Objective**: I implemented a simple classifier to predict weather outcomes using the `weather_forecast.csv` dataset.
  
#### Lab 3: Classifier on PIMA Diabetes Dataset
**Objective**: In this lab, I studied and implemented classifiers on the PIMA Indian Diabetes dataset, exploring the impact of different hyperparameters on model performance.

#### Lab 4: Feed Forward Neural Network on MNIST
**Objective**: I built a feed-forward neural network to recognize handwritten digits from the MNIST dataset.

#### Lab 5: Activation Functions
**Objective**: This lab focuses on experimenting with activation functions (like ReLU, sigmoid, and tanh) and observing their roles in model performance.

#### Lab 6 & 7: Comparison of Deep and Shallow Autoencoders
**Objective**: I implemented and compared deep and shallow autoencoders using the MNIST dataset, exploring how each approach handles dimensionality reduction and feature extraction.

#### Lab 8: Variational Autoencoder on MNIST
**Objective**: I created a Variational Autoencoder (VAE) to generate new samples from the MNIST dataset.

#### Lab 9: Experiment with LSTM
**Objective**: I experimented with Long Short-Term Memory (LSTM) networks for sequence prediction tasks, focusing on handling dependencies across time steps.

#### Lab 10: Compression on MNIST with Autoencoder
**Objective**: I performed compression on the MNIST dataset using an autoencoder model, aiming to reduce dimensionality while preserving key features.

#### Lab 11: Recurrent Neural Network
**Objective**: This lab involves building a Recurrent Neural Network (RNN) for sequence modeling, focusing on understanding the structure of RNNs and their applications.

#### Lab 12: Deep Convolutional GAN
**Objective**: I implemented a Deep Convolutional Generative Adversarial Network (DCGAN) to generate realistic images, learning how GANs function.

#### Lab 14: Transfer Learning with Pre-trained CNN
**Objective**: I used a pre-trained Convolutional Neural Network (CNN) as a feature extractor for image classification tasks, applying transfer learning techniques.

---

## Requirements

- Python 3.x
- Jupyter Notebook
- TensorFlow and Keras
- Numpy, Pandas, and Scikit-learn
- Matplotlib for visualizations

## Usage
1. Clone the repository:
   ```bash
   git clone https://github.com/Shrinjita/DeepLearning.git
   ```
2. Navigate to the repository and open Jupyter Notebook:
   ```bash
   cd DeepLearning
   jupyter notebook
   ```
3. Open and run the desired lab notebook.
